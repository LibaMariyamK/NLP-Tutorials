{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89aafad3",
   "metadata": {},
   "source": [
    "# Introduction to ELMo\n",
    "\n",
    "**ELMo (Embeddings from Language Models)** is a type of word representation that captures the **context of a word in a sentence**. Unlike traditional embeddings like Word2Vec or GloVe, which assign a single vector per word, ELMo generates **contextual embeddings** that change depending on the sentence.\n",
    "\n",
    "For example:\n",
    "\n",
    "* “I went to the **bank** to deposit money.”\n",
    "* “The river **bank** was full of trees.”\n",
    "\n",
    "Here, the word *bank* has different meanings. ELMo will generate different vectors for each context.\n",
    "\n",
    "\n",
    "**Contextual Embeddings:** ELMo takes into account the surrounding words to produce embeddings. The same word can have different vectors in different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b2016",
   "metadata": {},
   "source": [
    "# How ELMo Works\n",
    "\n",
    "**ELMo (Embeddings from Language Models)** is a contextual word representation method, meaning it considers the **context around a word** rather than giving a fixed vector. Here’s how it works step by step:\n",
    "\n",
    "### 1. Input Sentence\n",
    "\n",
    "You start with a sentence, for example:\n",
    "`\"I went to the bank to deposit money.\"`\n",
    "\n",
    "Each word in the sentence is first **converted into a basic vector** (usually character-level representation).\n",
    "\n",
    "\n",
    "### 2. Character-level Word Representation\n",
    "\n",
    "ELMo uses **character-level CNNs** (Convolutional Neural Networks) to generate an initial representation of each word.\n",
    "\n",
    "* This helps handle **rare or unknown words**, since it doesn’t rely solely on a prebuilt vocabulary.\n",
    "\n",
    "\n",
    "### 3. Bidirectional Language Model (biLM)\n",
    "\n",
    "ELMo uses a **two-layer bidirectional LSTM**:\n",
    "\n",
    "* **Forward LSTM:** Reads the sentence from start to end.\n",
    "* **Backward LSTM:** Reads the sentence from end to start.\n",
    "\n",
    "This way, ELMo captures context **from both sides** of each word.\n",
    "\n",
    "Example: In the sentence above, the word *bank* is understood differently depending on the words before and after it.\n",
    "\n",
    "\n",
    "### 4. Combining Representations\n",
    "\n",
    "The outputs from both LSTM directions are **combined** (usually by concatenation or a weighted sum) to get a **final embedding** for each word.\n",
    "\n",
    "* Each word now has a **1024-dimensional vector**.\n",
    "* These embeddings are **contextual**, so the same word in a different sentence will have a different vector.\n",
    "\n",
    "\n",
    "### 5. Using ELMo Embeddings\n",
    "\n",
    "ELMo embeddings can be used in any NLP model as features:\n",
    "\n",
    "* **Sentence classification** (e.g., sentiment analysis)\n",
    "* **Named Entity Recognition** (NER)\n",
    "* **Question answering**\n",
    "* **Text similarity**\n",
    "\n",
    "You don’t need to train ELMo yourself; you can use a **pretrained model** via TensorFlow Hub or AllenNLP.\n",
    " \n",
    "\n",
    "### Key Takeaways \n",
    "\n",
    "* ELMo captures **context** – same word → different embedding depending on sentence.\n",
    "* Uses **bidirectional LSTM + character-level CNNs**.\n",
    "* Embeddings are **deep** (from multiple layers) and **dynamic**.\n",
    "* Pretrained ELMo models are ready to use for most tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c7cb4",
   "metadata": {},
   "source": [
    "# Installing Required Libraries\n",
    "\n",
    "We will use **TensorFlow** and **TensorFlow Hub**, which provide pretrained ELMo models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfcc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a439f5",
   "metadata": {},
   "source": [
    "# Loading Pretrained ELMo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f799a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load pretrained ELMo model from TensorFlow Hub\n",
    "elmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "print(\"ELMo model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8618d6d",
   "metadata": {},
   "source": [
    "# Generating ELMo Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be58bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (1, 4, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Single sentence\n",
    "sentence = [\"ELMo embeddings are context-aware.\"]\n",
    "\n",
    "# Use ELMo model to generate embeddings\n",
    "elmo_embeddings = elmo_model.signatures['default'](tf.constant(sentence))['elmo']\n",
    "\n",
    "print(\"Shape of embeddings:\", elmo_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d414ea4",
   "metadata": {},
   "source": [
    "* Output shape: `(1, sequence_length, 1024)`\n",
    "\n",
    "  * `1` → number of sentences\n",
    "  * `sequence_length` → number of words in sentence\n",
    "  * `1024` → ELMo embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d08123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n",
      "tf.Tensor(\n",
      "[ 0.22764012 -0.05711553  0.1066011   0.5877569   0.12588204 -0.04812695\n",
      "  0.43702173  0.6652268   0.14785011  0.15778184], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Embedding for each word\n",
    "word_embedding = elmo_embeddings[0][0]  # Embedding for first word\n",
    "print(word_embedding.shape)  # (1024,)\n",
    "print(word_embedding[:10])   # First 10 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90cf6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 shape: (4, 1024)\n",
      "Sentence 2 shape: (4, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings for Multiple Sentences\n",
    "\n",
    "sentences = [\n",
    "    \"I love learning NLP.\",\n",
    "    \"ELMo embeddings capture context.\"\n",
    "]\n",
    "\n",
    "embeddings = elmo_model.signatures['default'](tf.constant(sentences))['elmo']\n",
    "\n",
    "for i, sentence_embedding in enumerate(embeddings):\n",
    "    print(f\"Sentence {i+1} shape: {sentence_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930be171",
   "metadata": {},
   "source": [
    "## Using ELMo Embeddings in a Task (Example: Sentence Similarity)\n",
    "\n",
    "We can compute similarity between sentences using **cosine similarity**.\n",
    "\n",
    "Higher values indicate more similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f00c533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.78412944\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I love coffee.\",\n",
    "    \"I enjoy drinking tea.\"\n",
    "]\n",
    "\n",
    "embeddings = elmo_model.signatures['default'](tf.constant(sentences))['elmo']\n",
    "\n",
    "# Average word embeddings to get sentence-level embedding\n",
    "sentence_embeddings = [np.mean(e.numpy(), axis=0) for e in embeddings]\n",
    "\n",
    "similarity = cosine_similarity([sentence_embeddings[0]], [sentence_embeddings[1]])\n",
    "print(\"Cosine similarity:\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465617ee",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    "* **ELMo vs Word2Vec/GloVe:** ELMo embeddings are dynamic and context-sensitive, unlike static embeddings from Word2Vec or GloVe.\n",
    "* **Shape:** Each word gets a 1024-dimensional vector.\n",
    "* **Input:** Sentences should be tokenized by space or using standard tokenizers.\n",
    "* **Pretrained model usage:** You don’t need to train ELMo from scratch unless doing advanced research.\n",
    "\n",
    "\n",
    "\n",
    "* ELMo provides **contextual word embeddings** using a **bidirectional LSTM**.\n",
    "* Each word gets a vector of size **1024**.\n",
    "* The embeddings can be used for many NLP tasks without retraining the model.\n",
    "* Using **TensorFlow Hub**, it’s very easy to integrate pretrained ELMo into Python projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
