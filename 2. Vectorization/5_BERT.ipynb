{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4909bcbe",
   "metadata": {},
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748df32",
   "metadata": {},
   "source": [
    "## What is BERT and Why Does It Matter?\n",
    "\n",
    "Unlike the embedding methods you've learned (Word2Vec, GloVe, ELMo), BERT understands context from both directions simultaneously. When you learned Word2Vec and GloVe, you saw that each word gets a fixed vector regardless of context. ELMo improved this by being context-aware, but BERT takes it further by looking at words from both left and right at the same time.\n",
    "\n",
    "Think about the word \"bank\" in these sentences:\n",
    "- \"I went to the bank to deposit money\"\n",
    "- \"I sat by the river bank\"\n",
    "\n",
    "BERT understands these differently because it reads the entire sentence bidirectionally, not just left-to-right.\n",
    "\n",
    "## The Foundation: Understanding Transformers Basics\n",
    "\n",
    "Before diving into BERT, you need to grasp the transformer architecture concept. Transformers use something called \"attention mechanism\" which allows the model to focus on relevant parts of the input when processing each word.\n",
    "\n",
    "The attention mechanism answers: \"When processing this word, which other words in the sentence should I pay attention to?\"\n",
    "\n",
    "For example, in \"The animal didn't cross the street because it was too tired\", the attention mechanism helps the model understand that \"it\" refers to \"animal\", not \"street\".\n",
    "\n",
    "## Key Concepts in BERT\n",
    "\n",
    "**Tokenization in BERT**\n",
    "\n",
    "BERT uses WordPiece tokenization, which breaks words into subwords. This handles unknown words better than word-level tokenization.\n",
    "\n",
    "Example:\n",
    "- \"playing\" might become [\"play\", \"##ing\"]\n",
    "- \"unhappiness\" might become [\"un\", \"##happiness\"]\n",
    "\n",
    "The \"##\" indicates a subword that continues from the previous token.\n",
    "\n",
    "**Special Tokens**\n",
    "\n",
    "BERT uses special tokens:\n",
    "- `[CLS]` - Added at the start of every sequence, used for classification tasks\n",
    "- `[SEP]` - Separates sentences in sentence-pair tasks\n",
    "- `[MASK]` - Used during training to mask words\n",
    "- `[PAD]` - Padding token for making sequences same length\n",
    "\n",
    "---\n",
    "\n",
    "### **Pre-training Tasks**\n",
    "\n",
    "BERT is trained **before use** on a large text corpus using two self-supervised tasks. These tasks help BERT understand both **word meaning** and **sentence relationships**.\n",
    "\n",
    "1. **Masked Language Modeling (MLM):**\n",
    "Instead of reading text only from left to right, BERT learns using full context. During training, about 15% of words in a sentence are hidden or altered. BERT’s job is to predict the original word by looking at the words **before and after** it.\n",
    "\n",
    "Example:\n",
    "\"The cat sat on the [MASK]\"\n",
    "BERT uses surrounding words to predict: **\"mat\"**\n",
    "\n",
    "This task teaches BERT grammar, word meaning, and how the same word can change meaning depending on context.\n",
    "\n",
    "2. **Next Sentence Prediction (NSP):**\n",
    "BERT also learns how sentences relate to each other. It is given two sentences together and must decide whether the second sentence logically follows the first one or not.\n",
    "\n",
    "Example (valid pair):\n",
    "Sentence A: \"She finished her exam.\"\n",
    "Sentence B: \"She felt relieved.\"\n",
    "\n",
    "Example (invalid pair):\n",
    "Sentence A: \"She finished her exam.\"\n",
    "Sentence B: \"The sky is blue.\"\n",
    "\n",
    "This task helps BERT understand sentence flow, which is important for question answering and sentence-pair tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6fd78",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de04a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for BERT\n",
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc324d6a",
   "metadata": {},
   "source": [
    "## Load a Pretrained BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fbc85",
   "metadata": {},
   "source": [
    "\n",
    "* Tokenizer → converts text to numbers\n",
    "* Model → converts numbers to embeddings\n",
    "* `eval()` → disables training behavior like dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f6d7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Set model to evaluation mode (important)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da53b09",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf918a83",
   "metadata": {},
   "source": [
    "* Text → tokens\n",
    "* Tokens → token IDs\n",
    "* Adds `[CLS]` and `[SEP]` automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22dc63b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2293,  4083, 17953,  2361,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"I love learning NLP\"\n",
    "\n",
    "# Tokenize and convert to tensors\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",   # PyTorch tensors\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d757ffe",
   "metadata": {},
   "source": [
    "## Generate BERT Embeddings\n",
    "\n",
    "This is where BERT actually works.\n",
    "\n",
    "What you get:\n",
    "\n",
    "* Shape → `(batch_size, sequence_length, 768)`\n",
    "* Each word has a **768-dimensional vector**\n",
    "* Same word in different sentences → different vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc639899",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # no gradients needed\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings\n",
    "last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f3eff",
   "metadata": {},
   "source": [
    "## Understand BERT Output Clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd06e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ce10e",
   "metadata": {},
   "source": [
    "* `batch_size` → number of sentences\n",
    "* `sequence_length` → number of tokens\n",
    "* `768` → embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1455523",
   "metadata": {},
   "source": [
    "## Sentence Embedding \n",
    "\n",
    "You usually need **one vector per sentence**, not per word.\n",
    "\n",
    "### CLS Token Embedding (Simple & Common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c369fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# CLS token is the first token\n",
    "sentence_embedding = last_hidden_state[:, 0, :]\n",
    "\n",
    "print(sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c20b5",
   "metadata": {},
   "source": [
    "This gives:\n",
    "\n",
    "* One **768D vector**\n",
    "* Represents the entire sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b749653",
   "metadata": {},
   "source": [
    "## Compare Meaning of Words (Context Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c188bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I went to the bank to deposit money\",\n",
    "    \"The river bank was beautiful\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731f73b",
   "metadata": {},
   "source": [
    "* Word **“bank”** gets **different embeddings**\n",
    "* This is why BERT is powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28af01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.5034990310668945\n"
     ]
    }
   ],
   "source": [
    "# Extract embedding vectors for \"bank\"\n",
    "\n",
    "bank_indices = [5, 3]\n",
    "bank_vec_1 = embeddings[0, bank_indices[0], :]\n",
    "bank_vec_2 = embeddings[1, bank_indices[1], :]\n",
    "\n",
    "# Compare the embeddings numerically - cosine similarity\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(bank_vec_1, bank_vec_2, dim=0)\n",
    "print(\"Cosine similarity:\", similarity.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b9696",
   "metadata": {},
   "source": [
    "## Downstream Task: Text Classification (Basic Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Classification Model\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2   # binary classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a15db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2607,  2003,  2200, 14044,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Input for Classification\n",
    "\n",
    "text = \"This course is very helpful\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47482647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Get Prediction (No Training)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc564b64",
   "metadata": {},
   "source": [
    "This is **inference only**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e13d1f",
   "metadata": {},
   "source": [
    "# Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff2968",
   "metadata": {},
   "source": [
    "1. **Load a pretrained BERT QA model**\n",
    "\n",
    "   * `deepset/bert-base-uncased-squad2` is already fine-tuned on a QA dataset (SQuAD2).\n",
    "   * This means it **can find answers in a context paragraph** without further training.\n",
    "\n",
    "2. **Take user input**\n",
    "\n",
    "   * The user provides a **context paragraph** and a **question**.\n",
    "\n",
    "3. **Prepare input for BERT**\n",
    "\n",
    "   * The question and context are tokenized together.\n",
    "   * Special tokens `[CLS]` (start) and `[SEP]` (separator) are added.\n",
    "   * Attention masks and token type IDs are created to let BERT know which tokens belong to the question and which belong to the context.\n",
    "\n",
    "4. **Predict answer span**\n",
    "\n",
    "   * BERT outputs **start logits** and **end logits** for each token position in the context.\n",
    "   * The model chooses the token with the **highest start logit** and the **highest end logit** as the answer span.\n",
    "\n",
    "5. **Extract the answer**\n",
    "\n",
    "   * The tokens between the predicted start and end positions are converted back to text using the tokenizer.\n",
    "   * If the model predicts an invalid span (end < start), it returns `\"No Answer Found\"`.\n",
    "\n",
    "6. **Compute confidence**\n",
    "\n",
    "   * Softmax probabilities of the start and end positions are averaged to give a simple **confidence score** for the predicted answer.\n",
    "\n",
    "7. **Display result**\n",
    "\n",
    "   * Prints the context, question, answer, and confidence.\n",
    "\n",
    "---\n",
    "\n",
    "### How it’s implemented\n",
    "\n",
    "* **Transformers library** handles all tokenization and model operations.\n",
    "* **`BertTokenizer`** converts text to token IDs and maps tokens back to words.\n",
    "* **`BertForQuestionAnswering`** contains BERT + a small linear layer that predicts start and end positions.\n",
    "* **`torch.no_grad()`** disables gradient calculations since we only want **inference**, not training.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> User gives context + question → BERT predicts start and end positions in the context → Tokens are decoded → Answer + confidence is shown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a6f4b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: What are Newton’s Laws of Motion? An object at rest remains at rest, and an object in motion remains in motion at constant speed and in a straight line unless acted on by an unbalanced force. The acceleration of an object depends on the mass of the object and the amount of force applied. Whenever one object exerts a force on another object, the second object exerts an equal and opposite on the first. Sir Isaac Newton worked in many areas of mathematics and physics. He developed the theories of gravitation in 1666 when he was only 23 years old. In 1686, he presented his three laws of motion in the “Principia Mathematica Philosophiae Naturalis.”  By developing his three laws of motion, Newton revolutionized science. Newton’s laws together with Kepler’s Laws explained why planets move in elliptical orbits rather than in circles\n",
      "\n",
      "Question: in which paper newton introduced laws of motion\n",
      "\n",
      "Answer: principia mathematica philosophiae naturalis\n",
      "Confidence: 54.24%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# Load pretrained QA model (fine-tuned on SQuAD2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-uncased-squad2\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-uncased-squad2\")\n",
    "model.eval()\n",
    "\n",
    "def prepare_qa_input(question, context):\n",
    "    \"\"\"Encode question and context for BERT QA\"\"\"\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        question,\n",
    "        context,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "def answer_question(question, context):\n",
    "    \"\"\"Extract answer from context for a given question\"\"\"\n",
    "    encoding = prepare_qa_input(question, context)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    token_type_ids = encoding['token_type_ids']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Get most likely start and end positions\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits)\n",
    "\n",
    "    # If model predicts end before start, answer is \"No Answer\"\n",
    "    if end_idx < start_idx:\n",
    "        return \"No Answer Found\", 0.0\n",
    "\n",
    "    answer_tokens = input_ids[0][start_idx:end_idx + 1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Compute confidence score\n",
    "    start_score = torch.max(torch.softmax(start_logits, dim=1)).item()\n",
    "    end_score = torch.max(torch.softmax(end_logits, dim=1)).item()\n",
    "    confidence = (start_score + end_score) / 2\n",
    "\n",
    "    return answer, confidence\n",
    "\n",
    "# Take user input\n",
    "context = input(\"Enter the context:\\n\")\n",
    "question = input(\"\\nEnter the question:\\n\")\n",
    "\n",
    "answer, confidence = answer_question(question, context)\n",
    "print(f\"\\nContext: {context}\")\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
